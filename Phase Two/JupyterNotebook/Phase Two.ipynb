{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7327ce1a-e849-404a-a21d-ef8ea0e0daa1",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Vehicle Detection Software</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dcec50-5cce-4012-880d-5b7ff4dbd3af",
   "metadata": {},
   "source": [
    "Project Prepared By:\n",
    "1. Youssef Abdelaziz Farrag ID : 1701732\n",
    "2. Merna Saeed Habib ID : 1701526"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7060f-0d35-4201-bb17-6f9c8935c428",
   "metadata": {},
   "source": [
    "This software was done using the following libraries:\n",
    "- OpenCV\n",
    "- Matplotlib\n",
    "- Numpy\n",
    "- MoviePy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e7bcf-38a4-4d9e-9847-5e9beb44ee50",
   "metadata": {},
   "source": [
    "## Program Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867bf4ec-335f-45cf-8c1a-db68ce3a08d6",
   "metadata": {},
   "source": [
    "1. HOG Feature Extraction on training data\n",
    "2. Training SVM Classifier\n",
    "3. Sliding Windows Technique\n",
    "4. Heatmap and Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3dd2f-2672-4771-bbd0-7544d8393f6d",
   "metadata": {},
   "source": [
    "## HOG Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f29099-3c7e-4db0-87dc-206fba56b6d3",
   "metadata": {},
   "source": [
    "In the first phase of the project, we perform HOG feature extraction on a set of labeled test images, labeled as vehicles and non-vehicles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24549e5-9050-480e-b9ec-4b500662f263",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <th><img src=\"kiti1.png\" width = 150></th>\n",
    "    <th><img src=\"kiti2.png\" width = 150></th>\n",
    "    <th><img src=\"kiti3.png\" width = 150></th>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f938b-dd9e-4bc7-9ff5-3fbfa1e45b20",
   "metadata": {},
   "source": [
    "<center> HOG Feature Extraction </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4432f702-56d9-417a-b278-52825a1f1d41",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <th><img src=\"carex1.png\" width = 150></th>\n",
    "    <th><img src=\"carex2.png\" width = 150></th>\n",
    "    <th><img src=\"carex3.png\" width = 150></th>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a7aa2-cdd4-49b6-808a-6fddcfd435fc",
   "metadata": {},
   "source": [
    "<center> Examples of cars from the training set of images.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fbfab5-0371-405e-8c28-64044aa85926",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <th><img src=\"nocarex1.png\" width = 150></th>\n",
    "    <th><img src=\"nocarex2.png\" width = 150></th>\n",
    "    <th><img src=\"nocarex3.png\" width = 150></th>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa8b07c-c42e-492c-bab9-cd37e6e227eb",
   "metadata": {},
   "source": [
    "<center> Examples of non-vehicles in the training set of images </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e5336-2948-414e-a822-c78a278d6c1e",
   "metadata": {},
   "source": [
    "This is done by the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1d2d41-722c-489b-b992-7a2ae0864762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    ''' Define a function to extract features from a list of images\n",
    "    This function call get_hog_features(), bin_spatial() and color_hist()\n",
    "    '''\n",
    "    # Create a list to append feature vectors to\n",
    "    vector_features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = mpimg.imread(file)\n",
    "        \n",
    "        # Apply color conversion if other than 'RGB'\n",
    "        feature_image = convert_RGB_color(image, color_space = color_space)     \n",
    "\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "        \n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        \n",
    "        if spatial_feat == True:\n",
    "            # Apply bin_spatial()\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        \n",
    "        vector_features.append(np.concatenate(file_features))\n",
    "    \n",
    "    # Return list of feature vectors\n",
    "    return vector_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b6d25-f764-4eb3-b389-4965ad7f0f6c",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f630f-e680-424e-bede-35a5dbc55baa",
   "metadata": {},
   "source": [
    "## Training the SVM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fcf3f7-5078-4188-b1bd-394308c73e79",
   "metadata": {},
   "source": [
    "In this phase, we train the SVM Classifier using the HOG features we obtained from the first phase, and using a YCrCb Color space for more accuracy. We used only 2760 images for training the SVM Classifier for better memory and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a9ad9-5d85-4f5d-8327-644309244f46",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a996f2-f83d-4b3c-85bf-26f463efd35f",
   "metadata": {},
   "source": [
    "## Sliding Window Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e20ad4-5851-4e6f-91e5-fc29746d4f0c",
   "metadata": {},
   "source": [
    "In this phase, we also implement a sliding window search as we did in the lane detection phase. The sliding window technique is the most commonly used method of pixel detection in image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d284d-31f0-4e7e-8a5b-cbc495901bdf",
   "metadata": {},
   "source": [
    "The problem in the sliding window search is that it consumes so much time, so we start the scaling of the image from 0.8 to 1.5 . The window then slides from top to bottom and left to right. The windows overlap to increase accuracy. It then selects the rectangles with a minimum number of pixels inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90acaae4-4e84-4696-943a-0968ad92ee92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cars(img, ystart, ystop, scale, svc, X_scaler, \n",
    "              orient, pix_per_cell, cell_per_block, spatial_size, hist_bins,\n",
    "              color_space,hog_channel,cells_per_step,idx,spatial_feat=True,hist_feat=False,clip = False):\n",
    "    \n",
    "    bbox_list=[]\n",
    "    out_img = np.copy(img)\n",
    "    \n",
    "    if clip:\n",
    "        img = img.astype(np.float32)/255  \n",
    "    \n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = convert_RGB_color(img_tosearch, color_space=color_space)    \n",
    "    \n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    if (hog_channel=='ALL'):\n",
    "        ch1 = ctrans_tosearch[:,:,0]\n",
    "        ch2 = ctrans_tosearch[:,:,1]\n",
    "        ch3 = ctrans_tosearch[:,:,2]\n",
    "    else:\n",
    "        ch1 = ctrans_tosearch[:,:,hog_channel]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell)-1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "    \n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    \n",
    "    nblocks_per_window = (window // pix_per_cell)-1 \n",
    "    \n",
    "    #cells_per_step = 2  # Instead of overlap, define how many cells to step (2)\n",
    "        \n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    if (hog_channel=='ALL'):\n",
    "        hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "        hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    #count = 0\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            #count +=1\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            if (hog_channel=='ALL'):\n",
    "                hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "                hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "                hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "            else:\n",
    "                hog_features = np.hstack((hog_feat1))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            if spatial_feat:\n",
    "                spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            if hist_feat:\n",
    "                hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            if hist_feat:\n",
    "                test_features = X_scaler.transform(np.hstack((\n",
    "                    hist_features,hog_features,spatial_features)).reshape(1, -1)) \n",
    "            else:\n",
    "                temp = np.hstack((hog_features,spatial_features)).reshape(1, -1)\n",
    "                temp = reduce_features(temp,idx)\n",
    "                test_features = X_scaler.transform(temp) \n",
    "            \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1:\n",
    "                \n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                cv2.rectangle(out_img,(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart),(0,0,1.),6) \n",
    "                bbox_list.append([(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart)])\n",
    "    \n",
    "    \n",
    "    return out_img,bbox_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04bd90-9633-4ddb-ad3a-c7db6e8db252",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cadf3a0-64b7-41d3-b083-c62589f0111b",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c803e60d-9c31-489c-be5e-806260173077",
   "metadata": {},
   "source": [
    "In this phase, we use a heatmap to identify the areas with the most concentration of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7068489f-322c-46cc-b70a-74eb98d86ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    # Return updated heatmap\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fb046-ddef-47ac-8d89-25545c97956a",
   "metadata": {},
   "source": [
    "After we identify the areas with most recurring pixels, we threshold the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4603c63-a743-4ca6-a87c-f24c2bace190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b53ce-3f6d-43b0-acbe-4ff69714140a",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <th><img src=\"heatmap1.jpg\" width = 400></th>\n",
    "    <th><img src=\"heatmap2.jpg\" width = 400></th>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5cf6bb-f29c-455e-ab20-f2158aab40e2",
   "metadata": {},
   "source": [
    "<center> Heatmap before and after thresholding </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2153668e-bed0-462b-a787-3861ffc34897",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa7f44-6ef6-4c83-8aa9-d4daee7c8a5d",
   "metadata": {},
   "source": [
    "## Detection of Vehicles and Bounding Box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb852ae-4af2-4ec9-833b-f85f39c68a1c",
   "metadata": {},
   "source": [
    "After the previous operations are done, and we have a heatmap of recurring pixels, we start by drawing a bounding box on the pixels detected, after that we average out nearby boxes and draw one big box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b9a717b-2094-44d1-aa2c-0d9a03615634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(img, bboxes, color=(0., 0., 1.), thick=6):\n",
    "    ''' Define a function to draw bounding boxes'''\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da3c66-b438-4f98-8f89-b8afa576141d",
   "metadata": {},
   "source": [
    "<center><img src='rawbox.jpg' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6054301f-0b22-45e2-beb7-229ff01cb34b",
   "metadata": {},
   "source": [
    "<center> Raw detection before averaging the boxes </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc9531-fbf8-41fe-ae78-1a6ecbdc62f1",
   "metadata": {},
   "source": [
    "<center><img src='final.jpg' width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f875c-d38c-4149-916e-3b49bba19d78",
   "metadata": {},
   "source": [
    "<center> Final car position, after removing the unwanted pixels and averaging the boxes </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef58da8-7f7b-4583-893f-5d5331bfa96f",
   "metadata": {},
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
